<!DOCTYPE HTML>
<html lang="en" >
    
    <head>
        
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <title>Hardware Architecture | NeuralHDL</title>
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="generator" content="GitBook 2.5.2">
        
        
        <meta name="HandheldFriendly" content="true"/>
        <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
        <meta name="apple-mobile-web-app-capable" content="yes">
        <meta name="apple-mobile-web-app-status-bar-style" content="black">
        <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
        <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">
        
    <link rel="stylesheet" href="../gitbook/style.css">
    
        
        <link rel="stylesheet" href="../gitbook/plugins/gitbook-plugin-mermaid-2/bower_components/mermaid/dist/mermaid.css">
        
    
        
        <link rel="stylesheet" href="../gitbook/plugins/gitbook-plugin-highlight/website.css">
        
    
        
        <link rel="stylesheet" href="../gitbook/plugins/gitbook-plugin-search/search.css">
        
    
        
        <link rel="stylesheet" href="../gitbook/plugins/gitbook-plugin-fontsettings/website.css">
        
    
    

        
    
    
    <link rel="next" href="../docs/language.html" />
    
    
    <link rel="prev" href="../docs/basic-example.html" />
    

        <script src="../gitbook/plugins/gitbook-plugin-mermaid-2/bower_components/mermaid/dist/mermaid.min.js"></script><link rel="stylesheet" href="../gitbook/plugins/gitbook-plugin-mermaid-2/bower_components/mermaid/dist/mermaid.forest.css"></link>
    </head>
    <body>
        
        
    <div class="book" data-level="4" data-chapter-title="Hardware Architecture" data-filepath="docs/architecture.md" data-basepath=".." data-revision="Thu Jul 06 2017 07:33:38 GMT-0700 (PDT)">
    

<div class="book-summary">
    <nav role="navigation">
        <ul class="summary">
            
            
            
            

            

            
    
        <li class="chapter " data-level="0" data-path="index.html">
            
                
                    <a href="../index.html">
                
                        <i class="fa fa-check"></i>
                        
                        Introduction
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="1" data-path="docs/getting-started.html">
            
                
                    <a href="../docs/getting-started.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>1.</b>
                        
                        Getting Started
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="2" data-path="docs/background.html">
            
                
                    <a href="../docs/background.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.</b>
                        
                        Background
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="3" data-path="docs/basic-example.html">
            
                
                    <a href="../docs/basic-example.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>3.</b>
                        
                        Examples
                    </a>
            
            
        </li>
    
        <li class="chapter active" data-level="4" data-path="docs/architecture.html">
            
                
                    <a href="../docs/architecture.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>4.</b>
                        
                        Hardware Architecture
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="5" data-path="docs/language.html">
            
                
                    <a href="../docs/language.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>5.</b>
                        
                        Language
                    </a>
            
            
        </li>
    


            
            <li class="divider"></li>
            <li>
                <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
                    Published with GitBook
                </a>
            </li>
            
        </ul>
    </nav>
</div>

    <div class="book-body">
        <div class="body-inner">
            <div class="book-header" role="navigation">
    <!-- Actions Left -->
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="../" >NeuralHDL</a>
    </h1>
</div>

            <div class="page-wrapper" tabindex="-1" role="main">
                <div class="page-inner">
                
                
                    <section class="normal" id="section-">
                    
                        <h1 id="hardware-architecture">Hardware Architecture</h1>
<p>There are many different architectures possible for neural networks. This section defines the initial architecture chosen usign this toolset which minimizes complexity for the example design. Like most hardware designs the proper algorithmic solution varies depending
on the use case. There are also some faults and bad initial decisions for this architecture which were not found until later in the design effort so have been left there to be fixed/changed at a later time. </p>
<p>The worst decision was the use of floating point which was actually a new addition to this toolset. The initial thought would be it would simplify the design from an algorithmic perspective but in reality just led to issues in complicating debug. This is first on the list for removal for a practical design. Further discussion of architectural choices can be found at the bottom of this section. </p>
<h1 id="top-level-architecture">Top Level Architecture</h1>
<p>The top level architecture for this design consists of a </p>
<p>The building block for the design in terms of interfaces is shown below.  </p>
<div class="mermaid">
graph LR;
input--&gt;IFifo[Input Fifo]

Core--&gt;Output
error[Input Error]--&gt;ErrorFifo
Core--&gt;oerror[Output Error]
IFifo--&gt;Memory
ErrorFifo--&gt;Memory[Internal Memory]
Memory--&gt;Core
Control--&gt;Memory
</div>

<p>Each stage of the network has an interface which consists of the following main interfaces. 
All of the interfaces use a ready valid format to gate the data. 
Currently the inputs to the stages contain fifos for flow control with the outputs streaming even 
though the interfaces are ready valid. The assumption currently is that a full operation is completed 
before it is interrupted. </p>
<h3 id="operation">Operation</h3>
<p>The operation of the network is straightforward and does not have any external control. Each stage of the network does 3 basic operations which are done using the same hardware and are time multiplexed based on the ordering below. </p>
<ol>
<li>Error Back Propagation</li>
<li>Tap/Bias Updates</li>
<li>Feedforward Propagation</li>
</ol>
<p>This order of operations should lead to the maximum network throughput while minimizing memory access. This operation is shared to due to it&apos;s access of the same information from memory. Parallel operation is also possible but would require more complicated and higher rate memory access. While sharing this unit is possible, it is probalby more efficient to add parallel stages working on different data rather than attempting to speed up these operations. </p>
<h2 id="neural-network-stage">Neural Network Stage</h2>
<p>Each stage of the network is self contained block which contains a set of control as well as memory. A high level block diagram is shown below. </p>
<div class="mermaid">
graph LR
    Input--&gt;IF
    IF[Input Fifo]--&gt;DM[Data Memory]
    Error--&gt;EF[Error Fifo]
    EF--&gt;TM[Tap Memory]
    TM--&gt;Core
    DM--&gt;Core
    Core--&gt;Output
    Core--&gt;E[Error Out]
    BM[Bias Memory]--&gt;Core
</div>

<p>This architecture was selected for simplicity but is not required. Sharing between stages as well as setting up resource pools for sharing is possible. For the current use cases there didn&apos;t seem to be advantages due to the full loading of the memories. </p>
<h2 id="memory-layout">Memory Layout</h2>
<p>The memory for each stage is split into 3 separate memories based on their size and frequency of use. </p>
<ol>
<li>Data Memory</li>
<li>Bias Memory</li>
<li>Tap and Error Memory</li>
</ol>
<p>The memory size and depths have been calculated to keep the network fully utilized. Cutting down the depths of the network can cut down the utilization of the network. </p>
<h3 id="data-memory">Data Memory</h3>
<p>The data memory contains the input data to the stage. This information is required for both the feedforward operation of the stage and the error updates. </p>
<ol>
<li>The memory requires one memory read for each cycle the stage is in use. </li>
<li>The depth requirement is a function of the length of time for the error to propagate back</li>
</ol>
<h3 id="bias-memory">Bias Memory</h3>
<p>The bias memory contains the bias values from the network. </p>
<ol>
<li>The memory requires one memory read for each cycle the stage is in use. </li>
<li>The depth requirement is equivalent to the size of the network</li>
</ol>
<h3 id="tap-and-error-memory">Tap and Error Memory</h3>
<p>The tap and error memory are shared in a single unit to simplify the interface to the neural stage as well as limit the memories used. </p>
<ol>
<li>The memory requires one parallel read (number of neurons width) for each operation of the neural stage</li>
<li>The depth of the memory is equivalent to the size of the taps plus the required error storage. The error depth is small because this data is utlized as quickly as possible to minimize required memory usage in the network. </li>
</ol>
<h1 id="neural-core-architecture">Neural Core Architecture</h1>
<p>This block contains a set of Neurons and is designed in a way to support the 3 basic Network operations using the same structure 
driving the inputs slightly differently. The different algorithms are all Matrix operations so the structure is designed to handle the 3 
operations with slightly different orderings. </p>
<h2 id="feedforward-operation">Feedforward operation</h2>
<p>For feedforward the operation is just a simple matrix multiplication with a bias addition. The ordering of this block was selected to keep memory access at a minimal level and to keep the 
operations running at a lower rate (bias addition, non-linearity) to be time shared. A block diagram
of the operation is shown below. </p>
<div class="mermaid">
graph LR
    Add(+)
        Mult(*)

        subgraph MACN
        Mult--&gt;Add
        Data0--&gt;Mult
        Tap--&gt;Mult
        end

       subgraph MAC0
        Mult1(*)--&gt;Add1(+)
        DataN--&gt;Mult1
        Tap1--&gt;Mult1
        end

        subgraph DelayLine
        Add--&gt;Rn
        Add1--&gt;R0
        end

        Data--&gt;Data0
        Data--&gt;DataN
        R0 --&gt;BiasAdd(+)
        TapDelay--&gt;BiasAdd
        BiasAdd--&gt;Output
</div>

<p>The block contains N MAC units which are shared for the operations. The data is ordered in a way to share the accumulator and limit memory accesses. 
The access is shown in the table below for a matrix with K MAC units and N total operations. The operation
order is always done so that K total outputs are completely computed before starting the next calculation. 
This ordering allows all the lower rate elements to be shared. </p>
<table>
<thead>
<tr>
<th>Type</th>
<th style="text-align:center">0</th>
<th style="text-align:right">1</th>
<th style="text-align:right">K</th>
<th style="text-align:right">K+1</th>
<th style="text-align:right">N</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tap (Vector)</td>
<td style="text-align:center">T0</td>
<td style="text-align:right">T1</td>
<td style="text-align:right">TK</td>
<td style="text-align:right">T(K+1)</td>
<td style="text-align:right">TN</td>
</tr>
<tr>
<td>Data</td>
<td style="text-align:center">D0</td>
<td style="text-align:right">D1</td>
<td style="text-align:right">DK</td>
<td style="text-align:right">D0</td>
<td style="text-align:right">DK</td>
</tr>
<tr>
<td>Bias</td>
<td style="text-align:center">B0</td>
<td style="text-align:right">B1</td>
<td style="text-align:right">BK</td>
<td style="text-align:right">B0</td>
<td style="text-align:right">BK</td>
</tr>
</tbody>
</table>
<p>The advantage of the ordering above is that the data is accessed linearily to minimize access 
and the accumulator is always used to update the outputs avoiding the use of an adder tree. </p>
<h2 id="error-update-operation">Error Update operation</h2>
<p>The error update calculation uses the same structure as above with a slightly different input configuration 
to the MAC units and some addition glue logic to handle this operation. The error update operation is also
a matrix multiply operation but uses error as input to the matrix multiply instead of the taps. </p>
<p>The block diagram of the operation is shown below. </p>
<div class="mermaid">
graph LR
    Add(+)
    Mult(*)

    subgraph MACN
    Mult--&gt;Add
    Data0--&gt;Mult
    Tap--&gt;Mult
    end

   subgraph MAC0
    Mult1(*)--&gt;Add1(+)
    DataN--&gt;Mult1
    Tap1--&gt;Mult1
    end

    subgraph DelayLine
    Add--&gt;Rn
    Add1--&gt;R0
    end

    Data--&gt;Data0
    Data--&gt;DataN
    R0 --&gt;BiasAdd(+)
    Bias--&gt;BiasAdd
    BiasAdd--&gt;Nonlinearity
    Nonlinearity--&gt;Output
</div>

<p>The operation shown above is very similar to the feedforward operation with the following differences </p>
<ol>
<li>The tap input is the error. This is made more efficient by storing the tap and error in the same memory</li>
<li>The output of the block is parallel and is written to the tap memory</li>
<li>The bias update is not shown but is done in a similar fashion as the tap update using a parallel 
adder</li>
</ol>
<p>The ordering of the operations is shown below</p>
<table>
<thead>
<tr>
<th>Type</th>
<th style="text-align:center">0</th>
<th style="text-align:right">1</th>
<th style="text-align:right">K</th>
<th style="text-align:right">K+1</th>
<th style="text-align:right">N</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tap (Vector)</td>
<td style="text-align:center">E0</td>
<td style="text-align:right">E1</td>
<td style="text-align:right">EK</td>
<td style="text-align:right">E(K+1)</td>
<td style="text-align:right">EN</td>
</tr>
<tr>
<td>Data</td>
<td style="text-align:center">D0</td>
<td style="text-align:right">D1</td>
<td style="text-align:right">DK</td>
<td style="text-align:right">D0</td>
<td style="text-align:right">DK</td>
</tr>
<tr>
<td>Bias</td>
<td style="text-align:center">B0</td>
<td style="text-align:right">B1</td>
<td style="text-align:right">BK</td>
<td style="text-align:right">B0</td>
<td style="text-align:right">BK</td>
</tr>
</tbody>
</table>
<h2 id="back-propagation">Back Propagation</h2>
<p>Back propagation is also a matrix multiplication operation between the taps and the error and like 
the previous two updates uses the same basic structure and ordering. </p>
<div class="mermaid">
graph LR
    Add(+)
    Mult(*)

    subgraph MACN
    Mult--&gt;Add
    Data0--&gt;Mult
    Tap--&gt;Mult
    end

   subgraph MAC0
    Mult1(*)--&gt;Add1(+)
    DataN--&gt;Mult1
    Tap1--&gt;Mult1
    end

    subgraph DelayLine
    Add--&gt;Rn
    Add1--&gt;R0
    end

    Data--&gt;Data0
    Data--&gt;DataN
    R0 --&gt;BiasAdd(+)
    Bias--&gt;BiasAdd
    BiasAdd--&gt;Nonlinearity
    Nonlinearity--&gt;Output
</div>

<p>There is an issue with this algorithm in that it uses the transpose of the taps which slightly complicates the memory access. To get around 
this problem while keeping the same structure the taps are accessed from memory in an offset fashion 
which is shown in the table below. </p>
<table>
<thead>
<tr>
<th>Type</th>
<th style="text-align:center">0</th>
<th style="text-align:right">1</th>
<th style="text-align:right">K</th>
<th style="text-align:right">K+1</th>
<th style="text-align:right">N</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tap0</td>
<td style="text-align:center">T0</td>
<td style="text-align:right">T1</td>
<td style="text-align:right">TK</td>
<td style="text-align:right">T(K+1)</td>
<td style="text-align:right">TN</td>
</tr>
<tr>
<td>Tap1</td>
<td style="text-align:center">T1</td>
<td style="text-align:right">T2</td>
<td style="text-align:right">T0</td>
<td style="text-align:right">T(K+1)</td>
<td style="text-align:right">TN</td>
</tr>
<tr>
<td>TapK</td>
<td style="text-align:center">TK</td>
<td style="text-align:right">T0</td>
<td style="text-align:right">T1</td>
<td style="text-align:right">T(K+1)</td>
<td style="text-align:right">TN</td>
</tr>
<tr>
<td>Data0</td>
<td style="text-align:center">E0</td>
<td style="text-align:right">E1</td>
<td style="text-align:right">EK</td>
<td style="text-align:right">..</td>
<td style="text-align:right">..</td>
</tr>
<tr>
<td>Data1</td>
<td style="text-align:center">E1</td>
<td style="text-align:right">E0</td>
<td style="text-align:right">..</td>
<td style="text-align:right">..</td>
<td style="text-align:right">..</td>
</tr>
<tr>
<td>DataK</td>
<td style="text-align:center">E2</td>
<td style="text-align:right">E2</td>
<td style="text-align:right">..</td>
<td style="text-align:right">..</td>
<td style="text-align:right">..</td>
</tr>
</tbody>
</table>
<h1 id="future-directions-for-architecture">Future Directions for Architecture</h1>

                    
                    </section>
                
                
                </div>
            </div>
        </div>

        
        <a href="../docs/basic-example.html" class="navigation navigation-prev " aria-label="Previous page: Examples"><i class="fa fa-angle-left"></i></a>
        
        
        <a href="../docs/language.html" class="navigation navigation-next " aria-label="Next page: Language"><i class="fa fa-angle-right"></i></a>
        
    </div>
</div>

        
<script src="../gitbook/app.js"></script>

    
    <script src="../gitbook/plugins/gitbook-plugin-mermaid-2/plugin.js"></script>
    

    
    <script src="../gitbook/plugins/gitbook-plugin-search/lunr.min.js"></script>
    

    
    <script src="../gitbook/plugins/gitbook-plugin-search/search.js"></script>
    

    
    <script src="../gitbook/plugins/gitbook-plugin-sharing/buttons.js"></script>
    

    
    <script src="../gitbook/plugins/gitbook-plugin-fontsettings/buttons.js"></script>
    

    
    <script src="../gitbook/plugins/gitbook-plugin-livereload/plugin.js"></script>
    

<script>
require(["gitbook"], function(gitbook) {
    var config = {"mermaid-2":{"theme":"forest"},"highlight":{},"search":{},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"livereload":{}};
    gitbook.start(config);
});
</script>

        
    </body>
    
</html>
